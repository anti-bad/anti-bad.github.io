---
layout: default
title: Anti-BAD Challenge
---

**Welcome to the Anti-BAD Challenge, an official IEEE SaTML 2026 competition.**

This challenge aims to advance defenses against backdoor attacks in Large Language Models (LLMs), focusing on post-trained scenarios where a model's provenance is opaque. While publicly shared LLMs are increasingly adopted, their inaccessible training processes and uncertain data integrity pose significant security risks, prompting the need for robust solutions to ensure safe and trustworthy deployment.

The competition invites participants to develop lightweight, effective methods that safeguard model behavior without access to training data or prior knowledge of the backdoor. To evaluate generalizability, Anti-BAD features three tracks reflecting core LLM applications: **generation, classification, and multilingual tasks.** These provide a benchmark grounded in real-world use.

## News

Any updates will be announced gradually.

## Overview

The **Anti-BAD: An Anti-Backdoor Challenge for Post-Trained Large Language Models** is a competition designed to address the growing security concerns in the era of widespread LLM deployment.

As obtaining and deploying language models from public hubs like HuggingFace becomes common practice, the risk of backdoor attacks increases. Malicious attackers can inject backdoors into models through fine-tuning on poisoned datasets or model editing, then publish these compromised models. The lack of transparency and model verification in current practices raises significant security concerns.

This challenge focuses on practical constraints faced by end-users in realistic deployment scenarios. Unlike existing defenses that require large-scale data and compute resources, Anti-BAD encourages the development of lightweight, generalizable strategies for securing LLMs under constrained post-training settings with minimal data and training assumptions.

The competition is designed to foster constrained post-training settings with minimal data and training assumptions. It encourages defenses—whether adapting traditional methods or introducing novel ones—to evaluate their effectiveness under a fair benchmark. To broaden its impact, the challenge spans widely used LLM scenarios, including generation, classification, and multilingual tasks.

## Important Dates

- **Competition registration opens:** Oct 21, 2025
- **Development phase starts:** Nov 7, 2025  
- **Test phase starts:** Feb 1, 2026
- **Test phase ends:** Feb 7, 2026
- **Final evaluation and ranking announcement:** Feb 8, 2026

## Organizers

Information about organizers will be updated soon.

## Contact

For questions and inquiries, please contact us at: [antibad-organisers@googlegroups.com](mailto:antibad-organisers@googlegroups.com)